{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import FlagEmbedding\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'id': '1311.5998'}, page_content='Title: A brief network analysis of Artificial Intelligence publication\\nAuthor: Yunpeng Li, Jie Liu, Yong Deng\\nAbstract:   In this paper, we present an illustration to the history of Artificial\\nIntelligence(AI) with a statistical analysis of publish since 1940. We\\ncollected and mined through the IEEE publish data base to analysis the\\ngeological and chronological variance of the activeness of research in AI. The\\nconnections between different institutes are showed. The result shows that the\\nleading community of AI research are mainly in the USA, China, the Europe and\\nJapan. The key institutes, authors and the research hotspots are revealed. It\\nis found that the research institutes in the fields like Data Mining, Computer\\nVision, Pattern Recognition and some other fields of Machine Learning are quite\\nconsistent, implying a strong interaction between the community of each field.\\nIt is also showed that the research of Electronic Engineering and Industrial or\\nCommercial applications are very active in California. Japan is also publishing\\na lot of papers in robotics. Due to the limitation of data source, the result\\nmight be overly influenced by the number of published articles, which is to our\\nbest improved by applying network keynode analysis on the research community\\ninstead of merely count the number of publish.\\n\\nURL: https://arxiv.org/pdf/1311.5998'), Document(metadata={'id': '2204.09719'}, page_content='Title: Recent Progress in Conversational AI\\nAuthor: Zijun Xue, Ruirui Li, Mingda Li\\nAbstract:   Conversational artificial intelligence (AI) is becoming an increasingly\\npopular topic among industry and academia. With the fast development of neural\\nnetwork-based models, a lot of neural-based conversational AI system are\\ndeveloped. We will provide a brief review of the recent progress in the\\nConversational AI, including the commonly adopted techniques, notable works,\\nfamous competitions from academia and industry and widely used datasets.\\n\\nURL: https://arxiv.org/pdf/2204.09719'), Document(metadata={'id': '2208.04714'}, page_content='Title: The History of AI Rights Research\\nAuthor: Jamie Harris\\nAbstract:   This report documents the history of research on AI rights and other moral\\nconsideration of artificial entities. It highlights key intellectual influences\\non this literature as well as research and academic discussion addressing the\\ntopic more directly. We find that researchers addressing AI rights have often\\nseemed to be unaware of the work of colleagues whose interests overlap with\\ntheir own. Academic interest in this topic has grown substantially in recent\\nyears; this reflects wider trends in academic research, but it seems that\\ncertain influential publications, the gradual, accumulating ubiquity of AI and\\nrobotic technology, and relevant news events may all have encouraged increased\\nacademic interest in this specific topic. We suggest four levers that, if\\npulled on in the future, might increase interest further: the adoption of\\npublication strategies similar to those of the most successful previous\\ncontributors; increased engagement with adjacent academic fields and debates;\\nthe creation of specialized journals, conferences, and research institutions;\\nand more exploration of legal rights for artificial entities.\\n\\nURL: https://arxiv.org/pdf/2208.04714')]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../shared_models/BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=model_path, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "vector_store = FAISS.load_local(\"../arxiv_vector_store_cs\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"What is the latest research on AI?\"\n",
    "docs = vector_store.similarity_search(query, k=3)\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"https://api.minimax.chat/v1\"\n",
    "base_url = \"https://api.lingyiwanwu.com/v1\"\n",
    "remote_model_name = \"yi-lightning\"\n",
    "api_key = \"c4ed4d9021634ba2a992fe155b0eb65c\"\n",
    "\n",
    "# Load openai client\n",
    "client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "\n",
    "def rephrase_query(client, query):\n",
    "    \"\"\"将用户查询转换为适合arxiv的关键词\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=remote_model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a key information extractor and always respond with only one most relevant keyword or key phrase from the input for use in a search engine query. Focus on essential terms and maintain the original wording.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def process_rag_results(client, query, chunks, metadata=None):\n",
    "    \"\"\"处理RAG结果并生成回答\"\"\"\n",
    "    # 构建上下文\n",
    "    context = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        source = f\"Source {i+1}\"\n",
    "        if metadata and i < len(metadata):\n",
    "            source = metadata[i]\n",
    "        context.append(f\"{source}: {chunk}\")\n",
    "    \n",
    "    context_str = \"\\n\\n\".join(context)\n",
    "    \n",
    "    # 生成回答\n",
    "    response = client.chat.completions.create(\n",
    "        model=remote_model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an academic assistant. Please answer the question based on the provided literature fragments. When answering, please cite the source of the information.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: {query}\\n\\nContext:\\n{context_str}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def process_rag_results_stream(client, query, chunks, metadata=None):\n",
    "    \"\"\"处理RAG结果并生成回答\"\"\"\n",
    "    # 构建上下文\n",
    "    context = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        source = f\"Source {i+1}\"\n",
    "        if metadata and i < len(metadata):\n",
    "            source = metadata[i]\n",
    "        context.append(f\"{source}: {chunk}\")\n",
    "    \n",
    "    context_str = \"\\n\\n\".join(context)\n",
    "    \n",
    "    # 生成回答 (启用流式输出)\n",
    "    response = client.chat.completions.create(\n",
    "        model=remote_model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an academic assistant. Please answer the question based on the provided literature fragments. When answering, please cite the source of the information.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: {query}\\n\\nContext:\\n{context_str}\"\n",
    "            }\n",
    "        ],\n",
    "        stream=True  # 启用流式输出\n",
    "    )\n",
    "    \n",
    "    # 处理流式响应\n",
    "    full_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "    print()  # 添加换行\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  7.74it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Loading ../shared_models/BAAI/bge-reranker-v2-minicpm-layerwise requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\FlagEmbedding\\inference\\reranker\\decoder_only\\layerwise.py:118\u001b[0m, in \u001b[0;36mLayerWiseLLMReranker.__init__\u001b[1;34m(self, model_name_or_path, peft_path, use_fp16, use_bf16, query_instruction_for_rerank, query_instruction_format, passage_instruction_for_rerank, passage_instruction_format, cache_dir, trust_remote_code, devices, cutoff_layers, prompt, batch_size, query_max_length, max_length, normalize, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLayerWiseMiniCPMForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_bf16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\transformers\\modeling_utils.py:3960\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3951\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3953\u001b[0m     (\n\u001b[0;32m   3954\u001b[0m         model,\n\u001b[0;32m   3955\u001b[0m         missing_keys,\n\u001b[0;32m   3956\u001b[0m         unexpected_keys,\n\u001b[0;32m   3957\u001b[0m         mismatched_keys,\n\u001b[0;32m   3958\u001b[0m         offload_index,\n\u001b[0;32m   3959\u001b[0m         error_msgs,\n\u001b[1;32m-> 3960\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\transformers\\modeling_utils.py:4414\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4413\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 4414\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4416\u001b[0m \u001b[38;5;66;03m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[39;00m\n\u001b[0;32m   4417\u001b[0m \u001b[38;5;66;03m# matching the weights in the model.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\transformers\\modeling_utils.py:548\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_safetensors_available():\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;66;03m# Check format of the archive\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    549\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mmetadata()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file or directory: \"../shared_models/BAAI/bge-reranker-v2-minicpm-layerwise\\\\model-00002-of-00003.safetensors\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFlagEmbedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayerWiseFlagLLMReranker\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Rerank the results\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mLayerWiseFlagLLMReranker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../shared_models/BAAI/bge-reranker-v2-minicpm-layerwise\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\FlagEmbedding\\inference\\reranker\\decoder_only\\layerwise.py:125\u001b[0m, in \u001b[0;36mLayerWiseLLMReranker.__init__\u001b[1;34m(self, model_name_or_path, peft_path, use_fp16, use_bf16, query_instruction_for_rerank, query_instruction_format, passage_instruction_for_rerank, passage_instruction_format, cache_dir, trust_remote_code, devices, cutoff_layers, prompt, batch_size, query_max_length, max_length, normalize, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LayerWiseMiniCPMForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    119\u001b[0m         model_name_or_path,\n\u001b[0;32m    120\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    121\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    122\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16 \u001b[38;5;28;01mif\u001b[39;00m use_bf16 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_bf16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_path:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,peft_path)\n",
      "File \u001b[1;32mc:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:524\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 524\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    525\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    526\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    527\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    528\u001b[0m     code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[0;32m    529\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    532\u001b[0m )\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:979\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    977\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    978\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n\u001b[1;32m--> 979\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m    984\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Harry\\miniconda3\\envs\\rag\\lib\\site-packages\\transformers\\dynamic_module_utils.py:640\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[0;32m    637\u001b[0m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires you to execute the configuration file in that\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m     )\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[1;31mValueError\u001b[0m: Loading ../shared_models/BAAI/bge-reranker-v2-minicpm-layerwise requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import LayerWiseFlagLLMReranker\n",
    "# Rerank the results\n",
    "reranker = LayerWiseFlagLLMReranker(\"../shared_models/BAAI/bge-reranker-v2-minicpm-layerwise\", use_fp16=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"watermarked LLMs identified by crafted prompts\"\n",
      "Time taken for similarity search: 0.4483 seconds\n",
      "Yes, watermarked Large Language Models (LLMs) can be identified by crafted prompts, according to the literature. The paper by Aiwei Liu and colleagues (2024) introduces an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Their research shows that watermarked LLMs exhibit consistent biases under the same watermark key, leading to similar differences across prompts under different watermark keys. This makes it possible to identify watermarked LLMs using specifically crafted prompts. The experiments conducted by the authors demonstrate that almost all mainstream watermarking algorithms can be easily identified with their well-designed prompts, while maintaining a minimal false positive rate for non-watermarked LLMs (Liu et al., 2024).\n",
      "\n",
      "Furthermore, the work by Leonard Tang and colleagues (2023) also supports this finding by introducing baseline algorithms for identifying watermarking schemes in LLMs. They note that watermarked LLMs produce output distributions that diverge identifiably from standard models, which can be analyzed to detect the presence of watermarks. Their research formalizes the problem of identifying watermarks in LLMs and provides a framework for studying LLM watermarks and watermark detection (Tang et al., 2023).\n",
      "\n",
      "Additionally, Xingchi Li and colleagues (2024) focus on detecting watermarked texts from LLMs and propose a statistical method to segment published texts into watermarked and non-watermarked sub-strings. Their approach, built upon randomization tests and change point detection techniques, ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. This further supports the notion that crafted prompts and statistical methods can be used to identify watermarked LLMs (Li et al., 2024).\n",
      "\n",
      "In summary, the literature indicates that watermarked LLMs can indeed be identified by crafted prompts through various detection algorithms and statistical methods.\n",
      "\n",
      "References:\n",
      "1. Liu, A., Guan, S., Liu, Y., Pan, L., Zhang, Y., Fang, L., Wen, L., Yu, P. S., & Hu, X. (2024). Can Watermarked LLMs be Identified by Users via Crafted Prompts? arXiv preprint arXiv:2410.03168. https://arxiv.org/pdf/2410.03168\n",
      "2. Tang, L., Uberti, G., & Shlomi, T. (2023). Baselines for Identifying Watermarked Large Language Models. arXiv preprint arXiv:2305.18456. https://arxiv.org/pdf/2305.18456\n",
      "3. Li, X., Li, G., & Zhang, X. (2024). Segmenting Watermarked Texts From Language Models. arXiv preprint arXiv:2410.20670. https://arxiv.org/pdf/2410.20670\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "query = \"Can watermarked LLMs be identified by crafted prompts?\"\n",
    "rephrased_query = rephrase_query(client, query)\n",
    "print(rephrased_query)\n",
    "\n",
    "# calculate times for similarity search\n",
    "import time\n",
    "start_time = time.time()\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    rephrased_query,\n",
    "    k=10\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for similarity search: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# combine a list of [[rephrased_query, doc_content], ...]\n",
    "rerank_data = [[rephrased_query, doc.page_content] for doc in results]\n",
    "scores = reranker.compute_score(rerank_data)\n",
    "# get top 5 results' index\n",
    "top_indices = np.argsort(scores)[-5:][::-1]\n",
    "top_results = [results[i] for i in top_indices]\n",
    "\n",
    "# 准备chunks和metadata\n",
    "chunks = []\n",
    "metadata = []\n",
    "for doc, score in top_results:\n",
    "    chunks.append(doc.page_content)\n",
    "    metadata.append(f\"Score: {score:.4f}\")\n",
    "\n",
    "answer = process_rag_results_stream(client, query, chunks, metadata)\n",
    "# print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
